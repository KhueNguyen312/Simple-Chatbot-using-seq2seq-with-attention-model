{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq chatbot with Movie Corpus tf2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhueNguyen312/Simple-Chatbot-using-seq2seq-with-attention-model/blob/main/Seq2Seq_chatbot_with_Movie_Corpus_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSMZ4CPQnB-U"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import unicodedata\n",
        "import pickle\n",
        "import collections\n",
        "import itertools\n",
        "import sys\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5qMSgNS1aN7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b16f830b-d21d-4f06-d873-a287879749fb"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tN-504EvI6ov"
      },
      "source": [
        "MODE = 'train'\n",
        "num_epochs = 200\n",
        "embedding_size = 256\n",
        "rnn_size = 512\n",
        "batch_size = 64\n",
        "max_len = 100\n",
        "max_sample = 300000\n",
        "trained_model_path = '/content/gdrive/My Drive/Colab Notebooks/Trained Models/Seq2Seq_Chatbot/movie_corpus'\n",
        "file_path_movie_lines = '/content/gdrive/My Drive/Colab Notebooks/Dataset/movie_lines.txt'\n",
        "file_path_conversations = '/content/gdrive/My Drive/Colab Notebooks/Dataset/movie_conversations.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5hsm6xGd78a"
      },
      "source": [
        "class DataPreprocesscing:\n",
        "  def __init__(self,file_path_movie_lines,file_path_conversations,max_len = 40,max_sample = -1):\n",
        "    self.path_movie_lines = file_path_movie_lines\n",
        "    self.path_conversations = file_path_conversations\n",
        "    self.START = '<start> ' # pad token\n",
        "    self.END = ' <end>'   # pad token\n",
        "    self.max_sample = max_sample\n",
        "    self.max_len = max_len\n",
        "\n",
        "  #from tensorflow repair dataset\n",
        "  def load_conversations(self):\n",
        "    # dictionary of line id to text\n",
        "    id2line = {}\n",
        "    with open(self.path_movie_lines, errors='ignore') as file:\n",
        "      lines = file.readlines()\n",
        "    for line in lines:\n",
        "      parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "      id2line[parts[0]] = parts[4]\n",
        "\n",
        "    inputs, outputs = [], []\n",
        "    with open(self.path_conversations, 'r') as file:\n",
        "      lines = file.readlines()\n",
        "    for line in lines:\n",
        "      parts = line.replace('\\n', '').split(' +++$+++ ')\n",
        "      # get conversation in a list of line ID\n",
        "      conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
        "      for i in range(len(conversation) - 1):\n",
        "        inputs.append(self.normalize_string(id2line[conversation[i]]))\n",
        "        outputs.append(self.normalize_string(id2line[conversation[i + 1]]))\n",
        "        if len(inputs) >= self.max_sample:\n",
        "          return inputs, outputs\n",
        "    return inputs, outputs\n",
        "\n",
        "  def unicode_to_ascii(self,s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "  def normalize_string(self,s):\n",
        "    s = s.lower().strip()\n",
        "    s = self.unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s\n",
        "\n",
        "  def tokenize(self,data):\n",
        "    data_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters='')\n",
        "    data_tokenizer.fit_on_texts(data)\n",
        "\n",
        "    tensor = data_tokenizer.texts_to_sequences(data)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                           padding='post')\n",
        "    return tensor, data_tokenizer\n",
        "\n",
        "  def process_data(self):\n",
        "    draw_inputs,draw_outputs = self.load_conversations()\n",
        "  \n",
        "    print('\\nSample from raw data: ')\n",
        "    print(draw_inputs[:5])\n",
        "    print(draw_outputs[:5])\n",
        "\n",
        "    tokenized_inputs,tokenized_outputs_in,tokenized_outputs_out = [],[],[]\n",
        "    for (sentence1, sentence2) in zip(draw_inputs, draw_outputs):\n",
        "      # tokenize sentence\n",
        "      sentence3 = sentence2 + self.END\n",
        "      sentence2 = self.START + sentence2\n",
        "      \n",
        "      # check tokenized sentence max length\n",
        "      if len(sentence1) <= self.max_len and len(sentence2) <= self.max_len + 1:\n",
        "        tokenized_inputs.append(sentence1)\n",
        "        tokenized_outputs_in.append(sentence2)\n",
        "        tokenized_outputs_out.append(sentence3)\n",
        "\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters='')\n",
        "    tokenizer.fit_on_texts(tokenized_inputs)\n",
        "    tokenizer.fit_on_texts(tokenized_outputs_in)\n",
        "    tokenizer.fit_on_texts(tokenized_outputs_out)\n",
        "\n",
        "    inputs = tokenizer.texts_to_sequences(tokenized_inputs)\n",
        "    dec_in = tokenizer.texts_to_sequences(tokenized_outputs_in)\n",
        "    dec_out = tokenizer.texts_to_sequences(tokenized_outputs_out)\n",
        "\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                           padding='post')\n",
        "    dec_in = tf.keras.preprocessing.sequence.pad_sequences(dec_in,\n",
        "                                                           padding='post')\n",
        "    dec_out = tf.keras.preprocessing.sequence.pad_sequences(dec_out,\n",
        "                                                           padding='post')\n",
        "\n",
        "    print('answer input sequences')\n",
        "    print(dec_in[:2])\n",
        "    print('answer output sequences')\n",
        "    print(dec_out[:2])\n",
        "    return inputs,dec_in,dec_out,tokenizer,tokenized_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru4lifsHJMdJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "d6149ccd-d327-45c2-f8b4-8bad24fa22fb"
      },
      "source": [
        "data = DataPreprocesscing(file_path_movie_lines,file_path_conversations,max_len,max_sample)\n",
        "inputs,dec_in,dec_out,tokenizer,draw_inputs = data.process_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sample from raw data: \n",
            "['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .', 'well i thought we d start with pronunciation if that s okay with you .', 'not the hacking and gagging and spitting part . please .', 'you re asking me out . that s so cute . what s your name again ?', 'no no it s my fault we didn t have a proper introduction ']\n",
            "['well i thought we d start with pronunciation if that s okay with you .', 'not the hacking and gagging and spitting part . please .', 'okay . . . then how bout we try out some french cuisine . saturday ? night ?', 'forget it .', 'cameron .']\n",
            "answer input sequences\n",
            "[[    4    29     7  8252    18 11204    18  6653   418     1   144     1\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    4    86     1     1     1    89    49   692    22   236    57   102\n",
            "   1094 11915     1  1443     3   159     3     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]]\n",
            "answer output sequences\n",
            "[[   29     7  8252    18 11204    18  6653   418     1   144     1     5\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [   86     1     1     1    89    49   692    22   236    57   102  1094\n",
            "  11915     1  1443     3   159     3     5     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lAIz4uMsR34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac865412-ec2e-48f4-99f5-7421ff742a68"
      },
      "source": [
        "draw_inputs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'well i thought we d start with pronunciation if that s okay with you .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q4s96MVqofX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df436d84-21c9-404e-83d2-f1999416d796"
      },
      "source": [
        "len(inputs[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72FmgxX6Or-r"
      },
      "source": [
        "#create dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (inputs, dec_in, dec_out))\n",
        "dataset = dataset.shuffle(len(inputs)).batch(\n",
        "    batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXHqZ_nawvoC"
      },
      "source": [
        "# dataset = tf.data.Dataset.from_tensor_slices(\n",
        "#     (data_en, data_fr_in, data_fr_out))\n",
        "# dataset = dataset.shuffle(20).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQWhc4GE5OrF"
      },
      "source": [
        "Divide the architecture of the model into 3 parts:\n",
        "\n",
        "-Encoder\n",
        "\n",
        "-Attention Layers\n",
        "\n",
        "-Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC8_146zz_-L"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self,vocab_size,embedding_size,rnn_size):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.rnn_size = rnn_size\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_size)\n",
        "    self.lstm = tf.keras.layers.LSTM(self.rnn_size,return_sequences=True,return_state=True)\n",
        "\n",
        "  def call(self,input_seq,hidden_state):\n",
        "    x = self.embedding(input_seq)\n",
        "    output,state_h,state_c = self.lstm(x,hidden_state)\n",
        "    return output,state_h,state_c\n",
        "  def initialize_hidden_state(self,batch_size):\n",
        "    return (tf.zeros([batch_size,self.rnn_size]),tf.zeros([batch_size,self.rnn_size]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfRSQftJKUiJ"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "encoder = Encoder(vocab_size, embedding_size, rnn_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPA5fulRKbgF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbe90a80-2fd1-478d-8bdd-a78b4f70bb2b"
      },
      "source": [
        "init_state = encoder.initialize_hidden_state(1)\n",
        "\n",
        "output,_,_ = encoder(tf.constant([[1,2,3]]),init_state)\n",
        "output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 3, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUog6LPQZ9PE"
      },
      "source": [
        "class LuongAttention(tf.keras.Model):\n",
        "  def __init__(self,rnn_size):\n",
        "    super(LuongAttention,self).__init__()\n",
        "    self.wa = tf.keras.layers.Dense(rnn_size)\n",
        "    \n",
        "  def call(self,decoder_output,encoder_output):\n",
        "    #Choose general attention score function\n",
        "    #decoder_output: (batch_size,1,rnn_size)\n",
        "    #encoder_output: (batch_size,max_len,rnn_size)\n",
        "    #score: (batch_size,1,max_len)\n",
        "    score = tf.matmul(decoder_output,self.wa(encoder_output),transpose_b=True)\n",
        "    alignment = tf.nn.softmax(score,axis=2)\n",
        "\n",
        "    #context vector c_t is the average sum of encoder output\n",
        "    context = tf.matmul(alignment,encoder_output)\n",
        "    return context, alignment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N40hACptd-Mq"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self,rnn_size,embedding_size,vocab_size):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.attention = LuongAttention(rnn_size)\n",
        "    self.rnn_size = rnn_size\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_size)\n",
        "    self.lstm = tf.keras.layers.LSTM(rnn_size,return_sequences=True,return_state=True)\n",
        "    \n",
        "    self.wc = tf.keras.layers.Dense(rnn_size,activation='tanh')\n",
        "    self.ws = tf.keras.layers.Dense(vocab_size)\n",
        "  def call(self,decoder_input,encoder_output,state):\n",
        "    #decoder_input's shape: (batch_size,1)\n",
        "    x = self.embedding(decoder_input)\n",
        "    #lstm_out's shape: (batch_size,1,rnn_size)\n",
        "    lstm_out,state_h,state_c = self.lstm(x,state)\n",
        "\n",
        "    #context vector's shape: (batch_size, 1, rnn_size)\n",
        "    #alignment vector's shape: (batch_size, 1, max_len)\n",
        "    context,alignment = self.attention(lstm_out,encoder_output)\n",
        "    lstm_out = tf.concat([tf.squeeze(context,1),tf.squeeze(lstm_out,1)],1)\n",
        "    lstm_out = self.wc(lstm_out)\n",
        "    logits = self.ws(lstm_out)\n",
        "    return logits,state_h,state_c,alignment\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td1XDp2bRZBl"
      },
      "source": [
        "decoder = Decoder(rnn_size, embedding_size,vocab_size )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgDFPvtVVUQP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ac0903b-1ebe-4742-cc7d-4ba1d9fc7580"
      },
      "source": [
        "initial_state = encoder.initialize_hidden_state(1)\n",
        "encoder_outputs = encoder(tf.constant([[2.1]]), initial_state)\n",
        "decoder_outputs = decoder(tf.constant(\n",
        "    [[2.1]]),encoder_outputs[0] ,encoder_outputs[1:])\n",
        "decoder_outputs[-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 1), dtype=float32, numpy=array([[[1.]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pD-fijeFlbE"
      },
      "source": [
        "def loss_function(targets, logits):\n",
        "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True)\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "    return loss\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R2tIvEki77w"
      },
      "source": [
        "@tf.function\n",
        "def train_step(input,target_in,target_out,init_hidden_state):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    en_output,en_state_h,en_state_c = encoder(input,init_hidden_state)\n",
        "    de_state_h = en_state_h\n",
        "    de_state_c = en_state_c\n",
        "    #Create a loop to iterate through the target sequences\n",
        "    for i in range(target_out.shape[1]):\n",
        "      #expand one dim because input must have shape of (batch_size,max_length)\n",
        "      de_input = tf.expand_dims(target_in[:,i],1)\n",
        "      logit,de_state_h,de_state_c,_ = decoder(de_input,en_output,(de_state_h,de_state_c))\n",
        "\n",
        "      loss += loss_function(target_out[:,i],logit)\n",
        "    \n",
        "  batch_loss = loss/target_out.shape[1]\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgNWkseCV09B"
      },
      "source": [
        "def predict(test_source_text=None):\n",
        "    if test_source_text is None:\n",
        "      test_source_text = draw_inputs[np.random.choice(len(draw_inputs))]\n",
        "    print(test_source_text)\n",
        "    test_source_seq = tokenizer.texts_to_sequences([test_source_text])\n",
        "    print(test_source_seq)\n",
        "\n",
        "    en_initial_states = encoder.initialize_hidden_state(1)\n",
        "    en_output,de_state_h, de_state_c = encoder(tf.constant(test_source_seq), en_initial_states)\n",
        "\n",
        "    de_input = tf.constant([[tokenizer.word_index['<start>']]])\n",
        "    out_words = []\n",
        "    alignments = []\n",
        "\n",
        "    while True:\n",
        "        de_output, de_state_h, de_state_c, alignment = decoder(\n",
        "            de_input, en_output, (de_state_h, de_state_c))\n",
        "        de_input = tf.expand_dims(tf.argmax(de_output, -1), 0)\n",
        "        out_words.append(tokenizer.index_word[de_input.numpy()[0][0]])\n",
        "        \n",
        "        alignments.append(alignment.numpy())\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= max_len:\n",
        "            break\n",
        "\n",
        "    print(' '.join(out_words))\n",
        "    return np.array(alignments), test_source_text.split(' '), out_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36Eyk5aBWw6o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe4f8f77-f7bf-4601-bd6b-02516c0a5c7e"
      },
      "source": [
        "if MODE == 'train':\n",
        "    for e in range(32,num_epochs):\n",
        "        en_initial_states = encoder.initialize_hidden_state(batch_size)\n",
        "        if e % 5 == 0:\n",
        "          encoder.save_weights(trained_model_path\n",
        "              + '/encoder_{}.h5'.format(e + 1))\n",
        "          decoder.save_weights(trained_model_path + '/decoder_{}.h5'.format(e + 1))\n",
        "        for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
        "            loss = train_step(source_seq, target_seq_in,target_seq_out, en_initial_states)\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "                    e + 1, batch, loss.numpy()))\n",
        "        try:\n",
        "            predict()\n",
        "\n",
        "            predict(\"How are you today ?\")\n",
        "        except Exception:\n",
        "            continue"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 33 Batch 0 Loss 0.8616\n",
            "Epoch 33 Batch 100 Loss 0.7755\n",
            "Epoch 33 Batch 200 Loss 0.7409\n",
            "Epoch 33 Batch 300 Loss 0.7668\n",
            "Epoch 33 Batch 400 Loss 0.8124\n",
            "Epoch 33 Batch 500 Loss 0.8205\n",
            "Epoch 33 Batch 600 Loss 0.9312\n",
            "Epoch 33 Batch 700 Loss 0.8254\n",
            "Epoch 33 Batch 800 Loss 1.0402\n",
            "Epoch 33 Batch 900 Loss 0.8243\n",
            "Epoch 33 Batch 1000 Loss 0.8470\n",
            "Epoch 33 Batch 1100 Loss 0.8332\n",
            "Epoch 33 Batch 1200 Loss 0.9130\n",
            "Epoch 33 Batch 1300 Loss 0.8066\n",
            "Epoch 33 Batch 1400 Loss 0.8548\n",
            "Epoch 33 Batch 1500 Loss 0.8899\n",
            "Epoch 33 Batch 1600 Loss 0.9844\n",
            "Epoch 33 Batch 1700 Loss 0.8340\n",
            "Epoch 33 Batch 1800 Loss 0.7787\n",
            "Epoch 33 Batch 1900 Loss 0.8952\n",
            "Epoch 33 Batch 2000 Loss 0.8075\n",
            "Epoch 33 Batch 2100 Loss 0.8169\n",
            "Epoch 33 Batch 2200 Loss 0.8229\n",
            "Epoch 33 Batch 2300 Loss 0.7983\n",
            "Epoch 33 Batch 2400 Loss 1.0184\n",
            "Epoch 33 Batch 2500 Loss 0.8987\n",
            "well thank you . anyway time for bed . or . . . sofa bed .\n",
            "[[58, 161, 2, 1, 377, 87, 32, 466, 1, 112, 1, 1, 1, 5690, 466, 1]]\n",
            "umm <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "gold . <end>\n",
            "Epoch 34 Batch 0 Loss 0.8302\n",
            "Epoch 34 Batch 100 Loss 0.8254\n",
            "Epoch 34 Batch 200 Loss 0.6968\n",
            "Epoch 34 Batch 300 Loss 0.7865\n",
            "Epoch 34 Batch 400 Loss 0.7786\n",
            "Epoch 34 Batch 500 Loss 0.8004\n",
            "Epoch 34 Batch 600 Loss 0.7707\n",
            "Epoch 34 Batch 700 Loss 0.6529\n",
            "Epoch 34 Batch 800 Loss 0.8497\n",
            "Epoch 34 Batch 900 Loss 0.7710\n",
            "Epoch 34 Batch 1000 Loss 0.8078\n",
            "Epoch 34 Batch 1100 Loss 0.8730\n",
            "Epoch 34 Batch 1200 Loss 0.8513\n",
            "Epoch 34 Batch 1300 Loss 0.7069\n",
            "Epoch 34 Batch 1400 Loss 0.8629\n",
            "Epoch 34 Batch 1500 Loss 0.8201\n",
            "Epoch 34 Batch 1600 Loss 0.8830\n",
            "Epoch 34 Batch 1700 Loss 0.7782\n",
            "Epoch 34 Batch 1800 Loss 0.8244\n",
            "Epoch 34 Batch 1900 Loss 0.9196\n",
            "Epoch 34 Batch 2000 Loss 0.7262\n",
            "Epoch 34 Batch 2100 Loss 0.8525\n",
            "Epoch 34 Batch 2200 Loss 0.7073\n",
            "Epoch 34 Batch 2300 Loss 0.7862\n",
            "Epoch 34 Batch 2400 Loss 0.7775\n",
            "Epoch 34 Batch 2500 Loss 0.9432\n",
            "well why the fuck would you want to lie anyway ?\n",
            "[[58, 63, 7, 176, 93, 2, 55, 9, 576, 377, 3]]\n",
            "you to go . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "gold . <end>\n",
            "Epoch 35 Batch 0 Loss 0.8210\n",
            "Epoch 35 Batch 100 Loss 0.8096\n",
            "Epoch 35 Batch 200 Loss 0.7242\n",
            "Epoch 35 Batch 300 Loss 0.8448\n",
            "Epoch 35 Batch 400 Loss 0.7216\n",
            "Epoch 35 Batch 500 Loss 0.7463\n",
            "Epoch 35 Batch 600 Loss 0.7712\n",
            "Epoch 35 Batch 700 Loss 0.6911\n",
            "Epoch 35 Batch 800 Loss 0.8413\n",
            "Epoch 35 Batch 900 Loss 0.7117\n",
            "Epoch 35 Batch 1000 Loss 0.8376\n",
            "Epoch 35 Batch 1100 Loss 0.8789\n",
            "Epoch 35 Batch 1200 Loss 0.8427\n",
            "Epoch 35 Batch 1300 Loss 0.7568\n",
            "Epoch 35 Batch 1400 Loss 0.7947\n",
            "Epoch 35 Batch 1500 Loss 0.8890\n",
            "Epoch 35 Batch 1600 Loss 0.6908\n",
            "Epoch 35 Batch 1700 Loss 0.7793\n",
            "Epoch 35 Batch 1800 Loss 0.6921\n",
            "Epoch 35 Batch 1900 Loss 0.7966\n",
            "Epoch 35 Batch 2000 Loss 0.8720\n",
            "Epoch 35 Batch 2100 Loss 0.6854\n",
            "Epoch 35 Batch 2200 Loss 0.7345\n",
            "Epoch 35 Batch 2300 Loss 0.8542\n",
            "Epoch 35 Batch 2400 Loss 0.7501\n",
            "Epoch 35 Batch 2500 Loss 0.7368\n",
            "but i ate your mom s pie .\n",
            "[[47, 6, 1526, 33, 317, 8, 2136, 1]]\n",
            "i m sorry sir . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "gold . <end>\n",
            "Epoch 36 Batch 0 Loss 0.7687\n",
            "Epoch 36 Batch 100 Loss 0.7033\n",
            "Epoch 36 Batch 200 Loss 0.7324\n",
            "Epoch 36 Batch 300 Loss 0.6673\n",
            "Epoch 36 Batch 400 Loss 0.7109\n",
            "Epoch 36 Batch 500 Loss 0.7389\n",
            "Epoch 36 Batch 600 Loss 0.7240\n",
            "Epoch 36 Batch 700 Loss 0.7008\n",
            "Epoch 36 Batch 800 Loss 0.6494\n",
            "Epoch 36 Batch 900 Loss 0.7883\n",
            "Epoch 36 Batch 1000 Loss 0.7428\n",
            "Epoch 36 Batch 1100 Loss 0.6840\n",
            "Epoch 36 Batch 1200 Loss 0.8321\n",
            "Epoch 36 Batch 1300 Loss 0.8142\n",
            "Epoch 36 Batch 1400 Loss 0.7641\n",
            "Epoch 36 Batch 1500 Loss 0.7349\n",
            "Epoch 36 Batch 1600 Loss 0.8314\n",
            "Epoch 36 Batch 1700 Loss 0.7668\n",
            "Epoch 36 Batch 1800 Loss 0.6660\n",
            "Epoch 36 Batch 1900 Loss 0.7874\n",
            "Epoch 36 Batch 2000 Loss 0.7634\n",
            "Epoch 36 Batch 2100 Loss 0.6977\n",
            "Epoch 36 Batch 2200 Loss 0.9036\n",
            "Epoch 36 Batch 2300 Loss 0.8099\n",
            "Epoch 36 Batch 2400 Loss 0.7156\n",
            "Epoch 36 Batch 2500 Loss 0.8042\n",
            "i want my fucking money \n",
            "[[6, 55, 34, 231, 177]]\n",
            "you re not . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "gold . <end>\n",
            "Epoch 37 Batch 0 Loss 0.6777\n",
            "Epoch 37 Batch 100 Loss 0.7255\n",
            "Epoch 37 Batch 200 Loss 0.7459\n",
            "Epoch 37 Batch 300 Loss 0.6922\n",
            "Epoch 37 Batch 400 Loss 0.7779\n",
            "Epoch 37 Batch 500 Loss 0.6504\n",
            "Epoch 37 Batch 600 Loss 0.6537\n",
            "Epoch 37 Batch 700 Loss 0.7307\n",
            "Epoch 37 Batch 800 Loss 0.6689\n",
            "Epoch 37 Batch 900 Loss 0.7293\n",
            "Epoch 37 Batch 1000 Loss 0.7103\n",
            "Epoch 37 Batch 1100 Loss 0.7525\n",
            "Epoch 37 Batch 1200 Loss 0.7546\n",
            "Epoch 37 Batch 1300 Loss 0.6878\n",
            "Epoch 37 Batch 1400 Loss 0.7607\n",
            "Epoch 37 Batch 1500 Loss 0.7126\n",
            "Epoch 37 Batch 1600 Loss 0.7928\n",
            "Epoch 37 Batch 1700 Loss 0.7007\n",
            "Epoch 37 Batch 1800 Loss 0.5444\n",
            "Epoch 37 Batch 1900 Loss 0.7364\n",
            "Epoch 37 Batch 2000 Loss 0.6977\n",
            "Epoch 37 Batch 2100 Loss 0.8082\n",
            "Epoch 37 Batch 2200 Loss 0.7053\n",
            "Epoch 37 Batch 2300 Loss 0.8357\n",
            "Epoch 37 Batch 2400 Loss 0.7992\n",
            "Epoch 37 Batch 2500 Loss 0.8495\n",
            "but you re a doctor . you kill people every day .\n",
            "[[47, 2, 28, 11, 334, 1, 2, 211, 154, 316, 202, 1]]\n",
            "i m sorry . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "gold <end>\n",
            "Epoch 38 Batch 0 Loss 0.6850\n",
            "Epoch 38 Batch 100 Loss 0.8191\n",
            "Epoch 38 Batch 200 Loss 0.6768\n",
            "Epoch 38 Batch 300 Loss 0.6802\n",
            "Epoch 38 Batch 400 Loss 0.6817\n",
            "Epoch 38 Batch 500 Loss 0.6605\n",
            "Epoch 38 Batch 600 Loss 0.6665\n",
            "Epoch 38 Batch 700 Loss 0.6660\n",
            "Epoch 38 Batch 800 Loss 0.6581\n",
            "Epoch 38 Batch 900 Loss 0.6948\n",
            "Epoch 38 Batch 1000 Loss 0.7001\n",
            "Epoch 38 Batch 1100 Loss 0.6855\n",
            "Epoch 38 Batch 1200 Loss 0.6660\n",
            "Epoch 38 Batch 1300 Loss 0.7497\n",
            "Epoch 38 Batch 1400 Loss 0.7191\n",
            "Epoch 38 Batch 1500 Loss 0.6552\n",
            "Epoch 38 Batch 1600 Loss 0.6488\n",
            "Epoch 38 Batch 1700 Loss 0.7826\n",
            "Epoch 38 Batch 1800 Loss 0.8475\n",
            "Epoch 38 Batch 1900 Loss 0.7804\n",
            "Epoch 38 Batch 2000 Loss 0.7295\n",
            "Epoch 38 Batch 2100 Loss 0.6832\n",
            "Epoch 38 Batch 2200 Loss 0.8340\n",
            "Epoch 38 Batch 2300 Loss 0.8559\n",
            "Epoch 38 Batch 2400 Loss 0.7526\n",
            "Epoch 38 Batch 2500 Loss 0.6589\n",
            "big eds .\n",
            "[[213, 9716, 1]]\n",
            "barrett male ? <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "? <end>\n",
            "Epoch 39 Batch 0 Loss 0.7005\n",
            "Epoch 39 Batch 100 Loss 0.6108\n",
            "Epoch 39 Batch 200 Loss 0.7518\n",
            "Epoch 39 Batch 300 Loss 0.6792\n",
            "Epoch 39 Batch 400 Loss 0.7422\n",
            "Epoch 39 Batch 500 Loss 0.6572\n",
            "Epoch 39 Batch 600 Loss 0.7370\n",
            "Epoch 39 Batch 700 Loss 0.7468\n",
            "Epoch 39 Batch 800 Loss 0.5546\n",
            "Epoch 39 Batch 900 Loss 0.6458\n",
            "Epoch 39 Batch 1000 Loss 0.6137\n",
            "Epoch 39 Batch 1100 Loss 0.6898\n",
            "Epoch 39 Batch 1200 Loss 0.7047\n",
            "Epoch 39 Batch 1300 Loss 0.7387\n",
            "Epoch 39 Batch 1400 Loss 0.8605\n",
            "Epoch 39 Batch 1500 Loss 0.6367\n",
            "Epoch 39 Batch 1600 Loss 0.5946\n",
            "Epoch 39 Batch 1700 Loss 0.6462\n",
            "Epoch 39 Batch 1800 Loss 0.7166\n",
            "Epoch 39 Batch 1900 Loss 0.7960\n",
            "Epoch 39 Batch 2000 Loss 0.7020\n",
            "Epoch 39 Batch 2100 Loss 0.6129\n",
            "Epoch 39 Batch 2200 Loss 0.7137\n",
            "Epoch 39 Batch 2300 Loss 0.8554\n",
            "Epoch 39 Batch 2400 Loss 0.7884\n",
            "Epoch 39 Batch 2500 Loss 0.8383\n",
            "i ll take care of this you guys leave the tip . and when i come back i want my book back .\n",
            "[[6, 41, 99, 261, 17, 30, 2, 290, 198, 7, 1528, 1, 18, 100, 6, 80, 88, 6, 55, 34, 517, 88, 1]]\n",
            "i m not going to take the whole thing out here . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "? <end>\n",
            "Epoch 40 Batch 0 Loss 0.6748\n",
            "Epoch 40 Batch 100 Loss 0.7727\n",
            "Epoch 40 Batch 200 Loss 0.6360\n",
            "Epoch 40 Batch 300 Loss 0.5876\n",
            "Epoch 40 Batch 400 Loss 0.6035\n",
            "Epoch 40 Batch 500 Loss 0.7561\n",
            "Epoch 40 Batch 600 Loss 0.7005\n",
            "Epoch 40 Batch 700 Loss 0.7947\n",
            "Epoch 40 Batch 800 Loss 0.6901\n",
            "Epoch 40 Batch 900 Loss 0.6212\n",
            "Epoch 40 Batch 1000 Loss 0.8332\n",
            "Epoch 40 Batch 1100 Loss 0.6880\n",
            "Epoch 40 Batch 1200 Loss 0.6981\n",
            "Epoch 40 Batch 1300 Loss 0.6622\n",
            "Epoch 40 Batch 1400 Loss 0.6804\n",
            "Epoch 40 Batch 1500 Loss 0.6510\n",
            "Epoch 40 Batch 1600 Loss 0.7742\n",
            "Epoch 40 Batch 1700 Loss 0.7036\n",
            "Epoch 40 Batch 1800 Loss 0.6793\n",
            "Epoch 40 Batch 1900 Loss 0.7768\n",
            "Epoch 40 Batch 2000 Loss 0.6721\n",
            "Epoch 40 Batch 2100 Loss 0.7011\n",
            "Epoch 40 Batch 2200 Loss 0.7389\n",
            "Epoch 40 Batch 2300 Loss 0.7991\n",
            "Epoch 40 Batch 2400 Loss 0.7029\n",
            "Epoch 40 Batch 2500 Loss 0.7589\n",
            "that night in the fire while my daddy was dyin . . . i saw mama up in her room with santos . . .\n",
            "[[15, 159, 21, 7, 540, 362, 34, 486, 39, 4237, 1, 1, 1, 6, 241, 842, 65, 21, 76, 266, 43, 2930, 1, 1, 1]]\n",
            "and you re a good teacher . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "? <end>\n",
            "Epoch 41 Batch 0 Loss 0.5880\n",
            "Epoch 41 Batch 100 Loss 0.6633\n",
            "Epoch 41 Batch 200 Loss 0.6082\n",
            "Epoch 41 Batch 300 Loss 0.6437\n",
            "Epoch 41 Batch 400 Loss 0.7515\n",
            "Epoch 41 Batch 500 Loss 0.6168\n",
            "Epoch 41 Batch 600 Loss 0.6699\n",
            "Epoch 41 Batch 700 Loss 0.7476\n",
            "Epoch 41 Batch 800 Loss 0.6438\n",
            "Epoch 41 Batch 900 Loss 0.7689\n",
            "Epoch 41 Batch 1000 Loss 0.6074\n",
            "Epoch 41 Batch 1100 Loss 0.6726\n",
            "Epoch 41 Batch 1200 Loss 0.6568\n",
            "Epoch 41 Batch 1300 Loss 0.7129\n",
            "Epoch 41 Batch 1400 Loss 0.6199\n",
            "Epoch 41 Batch 1500 Loss 0.6578\n",
            "Epoch 41 Batch 1600 Loss 0.7420\n",
            "Epoch 41 Batch 1700 Loss 0.6771\n",
            "Epoch 41 Batch 1800 Loss 0.6812\n",
            "Epoch 41 Batch 1900 Loss 0.7508\n",
            "Epoch 41 Batch 2000 Loss 0.6849\n",
            "Epoch 41 Batch 2100 Loss 0.6308\n",
            "Epoch 41 Batch 2200 Loss 0.6029\n",
            "Epoch 41 Batch 2300 Loss 0.5849\n",
            "Epoch 41 Batch 2400 Loss 0.8193\n",
            "Epoch 41 Batch 2500 Loss 0.6982\n",
            "the police ?\n",
            "[[7, 460, 3]]\n",
            "greens laval vu bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai bai\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "? <end>\n",
            "Epoch 42 Batch 0 Loss 0.6311\n",
            "Epoch 42 Batch 100 Loss 0.5991\n",
            "Epoch 42 Batch 200 Loss 0.6103\n",
            "Epoch 42 Batch 300 Loss 0.5916\n",
            "Epoch 42 Batch 400 Loss 0.5560\n",
            "Epoch 42 Batch 500 Loss 0.5610\n",
            "Epoch 42 Batch 600 Loss 0.6160\n",
            "Epoch 42 Batch 700 Loss 0.6616\n",
            "Epoch 42 Batch 800 Loss 0.7250\n",
            "Epoch 42 Batch 900 Loss 0.6367\n",
            "Epoch 42 Batch 1000 Loss 0.6266\n",
            "Epoch 42 Batch 1100 Loss 0.4883\n",
            "Epoch 42 Batch 1200 Loss 0.6280\n",
            "Epoch 42 Batch 1300 Loss 0.6518\n",
            "Epoch 42 Batch 1400 Loss 0.7836\n",
            "Epoch 42 Batch 1500 Loss 0.5829\n",
            "Epoch 42 Batch 1600 Loss 0.5506\n",
            "Epoch 42 Batch 1700 Loss 0.7174\n",
            "Epoch 42 Batch 1800 Loss 0.7276\n",
            "Epoch 42 Batch 1900 Loss 0.5495\n",
            "Epoch 42 Batch 2000 Loss 0.6168\n",
            "Epoch 42 Batch 2100 Loss 0.6379\n",
            "Epoch 42 Batch 2200 Loss 0.7120\n",
            "Epoch 42 Batch 2300 Loss 0.6571\n",
            "Epoch 42 Batch 2400 Loss 0.6939\n",
            "Epoch 42 Batch 2500 Loss 0.6152\n",
            "i have no doubt of it .\n",
            "[[6, 31, 25, 909, 17, 10, 1]]\n",
            "you re so damn ball . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "gold . <end>\n",
            "Epoch 43 Batch 0 Loss 0.5693\n",
            "Epoch 43 Batch 100 Loss 0.6577\n",
            "Epoch 43 Batch 200 Loss 0.6930\n",
            "Epoch 43 Batch 300 Loss 0.6836\n",
            "Epoch 43 Batch 400 Loss 0.6660\n",
            "Epoch 43 Batch 500 Loss 0.6123\n",
            "Epoch 43 Batch 600 Loss 0.5831\n",
            "Epoch 43 Batch 700 Loss 0.6305\n",
            "Epoch 43 Batch 800 Loss 0.6052\n",
            "Epoch 43 Batch 900 Loss 0.6324\n",
            "Epoch 43 Batch 1000 Loss 0.6407\n",
            "Epoch 43 Batch 1100 Loss 0.7244\n",
            "Epoch 43 Batch 1200 Loss 0.6783\n",
            "Epoch 43 Batch 1300 Loss 0.6159\n",
            "Epoch 43 Batch 1400 Loss 0.5561\n",
            "Epoch 43 Batch 1500 Loss 0.7063\n",
            "Epoch 43 Batch 1600 Loss 0.6280\n",
            "Epoch 43 Batch 1700 Loss 0.6549\n",
            "Epoch 43 Batch 1800 Loss 0.6858\n",
            "Epoch 43 Batch 1900 Loss 0.6899\n",
            "Epoch 43 Batch 2000 Loss 0.6150\n",
            "Epoch 43 Batch 2100 Loss 0.6912\n",
            "Epoch 43 Batch 2200 Loss 0.6513\n",
            "Epoch 43 Batch 2300 Loss 0.7242\n",
            "Epoch 43 Batch 2400 Loss 0.6666\n",
            "Epoch 43 Batch 2500 Loss 0.7577\n",
            "it s that time of year again . we should get a pretty good show out here .\n",
            "[[10, 8, 15, 87, 17, 426, 174, 1, 22, 128, 46, 11, 264, 77, 279, 57, 48, 1]]\n",
            "shit <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "helen <end>\n",
            "Epoch 44 Batch 0 Loss 0.6248\n",
            "Epoch 44 Batch 100 Loss 0.6702\n",
            "Epoch 44 Batch 200 Loss 0.5968\n",
            "Epoch 44 Batch 300 Loss 0.6264\n",
            "Epoch 44 Batch 400 Loss 0.6407\n",
            "Epoch 44 Batch 500 Loss 0.5809\n",
            "Epoch 44 Batch 600 Loss 0.6184\n",
            "Epoch 44 Batch 700 Loss 0.5803\n",
            "Epoch 44 Batch 800 Loss 0.6559\n",
            "Epoch 44 Batch 900 Loss 0.6505\n",
            "Epoch 44 Batch 1000 Loss 0.6678\n",
            "Epoch 44 Batch 1100 Loss 0.7263\n",
            "Epoch 44 Batch 1200 Loss 0.5952\n",
            "Epoch 44 Batch 1300 Loss 0.5727\n",
            "Epoch 44 Batch 1400 Loss 0.6181\n",
            "Epoch 44 Batch 1500 Loss 0.7056\n",
            "Epoch 44 Batch 1600 Loss 0.6645\n",
            "Epoch 44 Batch 1700 Loss 0.5833\n",
            "Epoch 44 Batch 1800 Loss 0.5768\n",
            "Epoch 44 Batch 1900 Loss 0.6489\n",
            "Epoch 44 Batch 2000 Loss 0.5155\n",
            "Epoch 44 Batch 2100 Loss 0.7043\n",
            "Epoch 44 Batch 2200 Loss 0.6871\n",
            "Epoch 44 Batch 2300 Loss 0.6079\n",
            "Epoch 44 Batch 2400 Loss 0.6655\n",
            "Epoch 44 Batch 2500 Loss 0.6445\n",
            "i was thinking about our conversation the other day what you said about choices .\n",
            "[[6, 39, 437, 44, 153, 1497, 7, 192, 202, 13, 2, 115, 44, 3852, 1]]\n",
            "yeah . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "you ? <end>\n",
            "Epoch 45 Batch 0 Loss 0.6493\n",
            "Epoch 45 Batch 100 Loss 0.6340\n",
            "Epoch 45 Batch 200 Loss 0.6418\n",
            "Epoch 45 Batch 300 Loss 0.5817\n",
            "Epoch 45 Batch 400 Loss 0.6356\n",
            "Epoch 45 Batch 500 Loss 0.5936\n",
            "Epoch 45 Batch 600 Loss 0.6732\n",
            "Epoch 45 Batch 700 Loss 0.5999\n",
            "Epoch 45 Batch 800 Loss 0.5780\n",
            "Epoch 45 Batch 900 Loss 0.6757\n",
            "Epoch 45 Batch 1000 Loss 0.6420\n",
            "Epoch 45 Batch 1100 Loss 0.6106\n",
            "Epoch 45 Batch 1200 Loss 0.6516\n",
            "Epoch 45 Batch 1300 Loss 0.6468\n",
            "Epoch 45 Batch 1400 Loss 0.7551\n",
            "Epoch 45 Batch 1500 Loss 0.5051\n",
            "Epoch 45 Batch 1600 Loss 0.6208\n",
            "Epoch 45 Batch 1700 Loss 0.6716\n",
            "Epoch 45 Batch 1800 Loss 0.6853\n",
            "Epoch 45 Batch 1900 Loss 0.6549\n",
            "Epoch 45 Batch 2000 Loss 0.5331\n",
            "Epoch 45 Batch 2100 Loss 0.5652\n",
            "Epoch 45 Batch 2200 Loss 0.7289\n",
            "Epoch 45 Batch 2300 Loss 0.6962\n",
            "Epoch 45 Batch 2400 Loss 0.5284\n",
            "Epoch 45 Batch 2500 Loss 0.5976\n",
            "thank you but it s missing something .\n",
            "[[161, 2, 47, 10, 8, 972, 106, 1]]\n",
            "ha . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "scotch of kick ? <end>\n",
            "Epoch 46 Batch 0 Loss 0.5280\n",
            "Epoch 46 Batch 100 Loss 0.5124\n",
            "Epoch 46 Batch 200 Loss 0.5554\n",
            "Epoch 46 Batch 300 Loss 0.5407\n",
            "Epoch 46 Batch 400 Loss 0.4814\n",
            "Epoch 46 Batch 500 Loss 0.6048\n",
            "Epoch 46 Batch 600 Loss 0.6480\n",
            "Epoch 46 Batch 700 Loss 0.5974\n",
            "Epoch 46 Batch 800 Loss 0.6570\n",
            "Epoch 46 Batch 900 Loss 0.5832\n",
            "Epoch 46 Batch 1000 Loss 0.6198\n",
            "Epoch 46 Batch 1100 Loss 0.5130\n",
            "Epoch 46 Batch 1200 Loss 0.5128\n",
            "Epoch 46 Batch 1300 Loss 0.7071\n",
            "Epoch 46 Batch 1400 Loss 0.6494\n",
            "Epoch 46 Batch 1500 Loss 0.6456\n",
            "Epoch 46 Batch 1600 Loss 0.5911\n",
            "Epoch 46 Batch 1700 Loss 0.6755\n",
            "Epoch 46 Batch 1800 Loss 0.5976\n",
            "Epoch 46 Batch 1900 Loss 0.5432\n",
            "Epoch 46 Batch 2000 Loss 0.5527\n",
            "Epoch 46 Batch 2100 Loss 0.7940\n",
            "Epoch 46 Batch 2200 Loss 0.6436\n",
            "Epoch 46 Batch 2300 Loss 0.5470\n",
            "Epoch 46 Batch 2400 Loss 0.5888\n",
            "Epoch 46 Batch 2500 Loss 0.6223\n",
            "i ll say whatever i want \n",
            "[[6, 41, 83, 434, 6, 55]]\n",
            "i m not sure . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "? <end>\n",
            "Epoch 47 Batch 0 Loss 0.5067\n",
            "Epoch 47 Batch 100 Loss 0.5706\n",
            "Epoch 47 Batch 200 Loss 0.6161\n",
            "Epoch 47 Batch 300 Loss 0.3986\n",
            "Epoch 47 Batch 400 Loss 0.5670\n",
            "Epoch 47 Batch 500 Loss 0.5870\n",
            "Epoch 47 Batch 600 Loss 0.4777\n",
            "Epoch 47 Batch 700 Loss 0.5736\n",
            "Epoch 47 Batch 800 Loss 0.5840\n",
            "Epoch 47 Batch 900 Loss 0.5470\n",
            "Epoch 47 Batch 1000 Loss 0.6011\n",
            "Epoch 47 Batch 1100 Loss 0.5479\n",
            "Epoch 47 Batch 1200 Loss 0.5918\n",
            "Epoch 47 Batch 1300 Loss 0.6126\n",
            "Epoch 47 Batch 1400 Loss 0.5358\n",
            "Epoch 47 Batch 1500 Loss 0.6340\n",
            "Epoch 47 Batch 1600 Loss 0.6175\n",
            "Epoch 47 Batch 1700 Loss 0.5756\n",
            "Epoch 47 Batch 1800 Loss 0.6503\n",
            "Epoch 47 Batch 1900 Loss 0.6517\n",
            "Epoch 47 Batch 2000 Loss 0.6093\n",
            "Epoch 47 Batch 2100 Loss 0.6533\n",
            "Epoch 47 Batch 2200 Loss 0.5944\n",
            "Epoch 47 Batch 2300 Loss 0.6855\n",
            "Epoch 47 Batch 2400 Loss 0.4849\n",
            "Epoch 47 Batch 2500 Loss 0.5872\n",
            "well you shall see .\n",
            "[[58, 2, 555, 78, 1]]\n",
            "shaft no <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "you ? <end>\n",
            "Epoch 48 Batch 0 Loss 0.5605\n",
            "Epoch 48 Batch 100 Loss 0.5130\n",
            "Epoch 48 Batch 200 Loss 0.5900\n",
            "Epoch 48 Batch 300 Loss 0.5562\n",
            "Epoch 48 Batch 400 Loss 0.6007\n",
            "Epoch 48 Batch 500 Loss 0.6195\n",
            "Epoch 48 Batch 600 Loss 0.5180\n",
            "Epoch 48 Batch 700 Loss 0.5816\n",
            "Epoch 48 Batch 800 Loss 0.6120\n",
            "Epoch 48 Batch 900 Loss 0.5968\n",
            "Epoch 48 Batch 1000 Loss 0.5843\n",
            "Epoch 48 Batch 1100 Loss 0.5289\n",
            "Epoch 48 Batch 1200 Loss 0.5775\n",
            "Epoch 48 Batch 1300 Loss 0.6133\n",
            "Epoch 48 Batch 1400 Loss 0.6047\n",
            "Epoch 48 Batch 1500 Loss 0.5570\n",
            "Epoch 48 Batch 1600 Loss 0.5372\n",
            "Epoch 48 Batch 1700 Loss 0.5044\n",
            "Epoch 48 Batch 1800 Loss 0.6133\n",
            "Epoch 48 Batch 1900 Loss 0.5800\n",
            "Epoch 48 Batch 2000 Loss 0.5959\n",
            "Epoch 48 Batch 2100 Loss 0.5657\n",
            "Epoch 48 Batch 2200 Loss 0.6543\n",
            "Epoch 48 Batch 2300 Loss 0.6107\n",
            "Epoch 48 Batch 2400 Loss 0.5336\n",
            "Epoch 48 Batch 2500 Loss 0.6766\n",
            "i had everything . once .\n",
            "[[6, 114, 223, 1, 356, 1]]\n",
            "i was just . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "snake <end>\n",
            "Epoch 49 Batch 0 Loss 0.6047\n",
            "Epoch 49 Batch 100 Loss 0.5833\n",
            "Epoch 49 Batch 200 Loss 0.6081\n",
            "Epoch 49 Batch 300 Loss 0.5349\n",
            "Epoch 49 Batch 400 Loss 0.6072\n",
            "Epoch 49 Batch 500 Loss 0.5847\n",
            "Epoch 49 Batch 600 Loss 0.5408\n",
            "Epoch 49 Batch 700 Loss 0.5408\n",
            "Epoch 49 Batch 800 Loss 0.5520\n",
            "Epoch 49 Batch 900 Loss 0.6563\n",
            "Epoch 49 Batch 1000 Loss 0.4776\n",
            "Epoch 49 Batch 1100 Loss 0.6411\n",
            "Epoch 49 Batch 1200 Loss 0.5428\n",
            "Epoch 49 Batch 1300 Loss 0.6235\n",
            "Epoch 49 Batch 1400 Loss 0.5120\n",
            "Epoch 49 Batch 1500 Loss 0.6275\n",
            "Epoch 49 Batch 1600 Loss 0.5809\n",
            "Epoch 49 Batch 1700 Loss 0.6071\n",
            "Epoch 49 Batch 1800 Loss 0.5133\n",
            "Epoch 49 Batch 1900 Loss 0.5237\n",
            "Epoch 49 Batch 2000 Loss 0.5487\n",
            "Epoch 49 Batch 2100 Loss 0.5461\n",
            "Epoch 49 Batch 2200 Loss 0.5705\n",
            "Epoch 49 Batch 2300 Loss 0.6121\n",
            "Epoch 49 Batch 2400 Loss 0.5248\n",
            "Epoch 49 Batch 2500 Loss 0.6202\n",
            "remember everyone ! only two weeks until your science projects are due .\n",
            "[[208, 480, 14, 140, 136, 636, 415, 33, 1879, 9348, 37, 1574, 1]]\n",
            "oh really ? <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "keeping poison to meet . <end>\n",
            "Epoch 50 Batch 0 Loss 0.5234\n",
            "Epoch 50 Batch 100 Loss 0.4832\n",
            "Epoch 50 Batch 200 Loss 0.5285\n",
            "Epoch 50 Batch 300 Loss 0.4784\n",
            "Epoch 50 Batch 400 Loss 0.5516\n",
            "Epoch 50 Batch 500 Loss 0.4998\n",
            "Epoch 50 Batch 600 Loss 0.5548\n",
            "Epoch 50 Batch 700 Loss 0.5127\n",
            "Epoch 50 Batch 800 Loss 0.5140\n",
            "Epoch 50 Batch 900 Loss 0.5811\n",
            "Epoch 50 Batch 1000 Loss 0.5971\n",
            "Epoch 50 Batch 1100 Loss 0.6493\n",
            "Epoch 50 Batch 1200 Loss 0.5903\n",
            "Epoch 50 Batch 1300 Loss 0.6180\n",
            "Epoch 50 Batch 1400 Loss 0.6146\n",
            "Epoch 50 Batch 1500 Loss 0.5197\n",
            "Epoch 50 Batch 1600 Loss 0.6787\n",
            "Epoch 50 Batch 1700 Loss 0.6607\n",
            "Epoch 50 Batch 1800 Loss 0.4696\n",
            "Epoch 50 Batch 1900 Loss 0.4586\n",
            "Epoch 50 Batch 2000 Loss 0.5734\n",
            "Epoch 50 Batch 2100 Loss 0.4917\n",
            "Epoch 50 Batch 2200 Loss 0.5337\n",
            "Epoch 50 Batch 2300 Loss 0.6137\n",
            "Epoch 50 Batch 2400 Loss 0.7551\n",
            "Epoch 50 Batch 2500 Loss 0.6403\n",
            "that s the maharajah that kid ? !\n",
            "[[15, 8, 7, 6793, 15, 370, 3, 14]]\n",
            "it s his own blood . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "poison you ? <end>\n",
            "Epoch 51 Batch 0 Loss 0.5283\n",
            "Epoch 51 Batch 100 Loss 0.5236\n",
            "Epoch 51 Batch 200 Loss 0.4531\n",
            "Epoch 51 Batch 300 Loss 0.5142\n",
            "Epoch 51 Batch 400 Loss 0.6086\n",
            "Epoch 51 Batch 500 Loss 0.4696\n",
            "Epoch 51 Batch 600 Loss 0.5343\n",
            "Epoch 51 Batch 700 Loss 0.6982\n",
            "Epoch 51 Batch 800 Loss 0.4309\n",
            "Epoch 51 Batch 900 Loss 0.5411\n",
            "Epoch 51 Batch 1000 Loss 0.6866\n",
            "Epoch 51 Batch 1100 Loss 0.4988\n",
            "Epoch 51 Batch 1200 Loss 0.6165\n",
            "Epoch 51 Batch 1300 Loss 0.4851\n",
            "Epoch 51 Batch 1400 Loss 0.5705\n",
            "Epoch 51 Batch 1500 Loss 0.5400\n",
            "Epoch 51 Batch 1600 Loss 0.5892\n",
            "Epoch 51 Batch 1700 Loss 0.5907\n",
            "Epoch 51 Batch 1800 Loss 0.6373\n",
            "Epoch 51 Batch 1900 Loss 0.6167\n",
            "Epoch 51 Batch 2000 Loss 0.5529\n",
            "Epoch 51 Batch 2100 Loss 0.6413\n",
            "Epoch 51 Batch 2200 Loss 0.5851\n",
            "Epoch 51 Batch 2300 Loss 0.5271\n",
            "Epoch 51 Batch 2400 Loss 0.5825\n",
            "Epoch 51 Batch 2500 Loss 0.5651\n",
            "here they come .\n",
            "[[48, 45, 80, 1]]\n",
            "you re right ! ! ! <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "snake i m sick ? <end>\n",
            "Epoch 52 Batch 0 Loss 0.4483\n",
            "Epoch 52 Batch 100 Loss 0.5121\n",
            "Epoch 52 Batch 200 Loss 0.4912\n",
            "Epoch 52 Batch 300 Loss 0.5203\n",
            "Epoch 52 Batch 400 Loss 0.4410\n",
            "Epoch 52 Batch 500 Loss 0.5109\n",
            "Epoch 52 Batch 600 Loss 0.4125\n",
            "Epoch 52 Batch 700 Loss 0.4593\n",
            "Epoch 52 Batch 800 Loss 0.5590\n",
            "Epoch 52 Batch 900 Loss 0.5475\n",
            "Epoch 52 Batch 1000 Loss 0.5709\n",
            "Epoch 52 Batch 1100 Loss 0.4191\n",
            "Epoch 52 Batch 1200 Loss 0.5423\n",
            "Epoch 52 Batch 1300 Loss 0.5366\n",
            "Epoch 52 Batch 1400 Loss 0.4576\n",
            "Epoch 52 Batch 1500 Loss 0.5176\n",
            "Epoch 52 Batch 1600 Loss 0.6042\n",
            "Epoch 52 Batch 1700 Loss 0.5960\n",
            "Epoch 52 Batch 1800 Loss 0.6351\n",
            "Epoch 52 Batch 1900 Loss 0.5721\n",
            "Epoch 52 Batch 2000 Loss 0.5403\n",
            "Epoch 52 Batch 2100 Loss 0.6250\n",
            "Epoch 52 Batch 2200 Loss 0.4752\n",
            "Epoch 52 Batch 2300 Loss 0.5709\n",
            "Epoch 52 Batch 2400 Loss 0.5055\n",
            "Epoch 52 Batch 2500 Loss 0.5580\n",
            "sun city . i ve been meaning to call you for months .\n",
            "[[1150, 670, 1, 6, 69, 97, 1336, 9, 143, 2, 32, 600, 1]]\n",
            "how d you see . . . ? <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "snake <end>\n",
            "Epoch 53 Batch 0 Loss 0.5046\n",
            "Epoch 53 Batch 100 Loss 0.5087\n",
            "Epoch 53 Batch 200 Loss 0.3834\n",
            "Epoch 53 Batch 300 Loss 0.4713\n",
            "Epoch 53 Batch 400 Loss 0.5694\n",
            "Epoch 53 Batch 500 Loss 0.4631\n",
            "Epoch 53 Batch 600 Loss 0.5501\n",
            "Epoch 53 Batch 700 Loss 0.5095\n",
            "Epoch 53 Batch 800 Loss 0.4599\n",
            "Epoch 53 Batch 900 Loss 0.5465\n",
            "Epoch 53 Batch 1000 Loss 0.5487\n",
            "Epoch 53 Batch 1100 Loss 0.5803\n",
            "Epoch 53 Batch 1200 Loss 0.4601\n",
            "Epoch 53 Batch 1300 Loss 0.5580\n",
            "Epoch 53 Batch 1400 Loss 0.6017\n",
            "Epoch 53 Batch 1500 Loss 0.5126\n",
            "Epoch 53 Batch 1600 Loss 0.5124\n",
            "Epoch 53 Batch 1700 Loss 0.5333\n",
            "Epoch 53 Batch 1800 Loss 0.5691\n",
            "Epoch 53 Batch 1900 Loss 0.3618\n",
            "Epoch 53 Batch 2000 Loss 0.4853\n",
            "Epoch 53 Batch 2100 Loss 0.4752\n",
            "Epoch 53 Batch 2200 Loss 0.5737\n",
            "Epoch 53 Batch 2300 Loss 0.6325\n",
            "Epoch 53 Batch 2400 Loss 0.4836\n",
            "Epoch 53 Batch 2500 Loss 0.5776\n",
            "i can go . you don t have to . you don t never go .\n",
            "[[6, 35, 66, 1, 2, 20, 12, 31, 9, 1, 2, 20, 12, 110, 66, 1]]\n",
            "oskar i ll take it . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "that . gold . <end>\n",
            "Epoch 54 Batch 0 Loss 0.3952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OttEjrJWGfUK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04187bfb-8e75-4957-9891-263c8de5188d"
      },
      "source": [
        "if MODE == 'train':\n",
        "    for e in range(51,num_epochs):\n",
        "        en_initial_states = encoder.initialize_hidden_state(batch_size)\n",
        "        if e % 5 == 0:\n",
        "          encoder.save_weights(trained_model_path\n",
        "              + '/encoder_{}.h5'.format(e + 1))\n",
        "          decoder.save_weights(trained_model_path + '/decoder_{}.h5'.format(e + 1))\n",
        "        for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
        "            loss = train_step(source_seq, target_seq_in,target_seq_out, en_initial_states)\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                print('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "                    e + 1, batch, loss.numpy()))\n",
        "        try:\n",
        "            predict()\n",
        "\n",
        "            predict(\"How are you today ?\")\n",
        "        except Exception:\n",
        "            continue"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 52 Batch 0 Loss 0.4185\n",
            "Epoch 52 Batch 100 Loss 0.4916\n",
            "Epoch 52 Batch 200 Loss 0.4382\n",
            "Epoch 52 Batch 300 Loss 0.5023\n",
            "Epoch 52 Batch 400 Loss 0.5779\n",
            "Epoch 52 Batch 500 Loss 0.5068\n",
            "Epoch 52 Batch 600 Loss 0.4444\n",
            "Epoch 52 Batch 700 Loss 0.4899\n",
            "Epoch 52 Batch 800 Loss 0.4954\n",
            "Epoch 52 Batch 900 Loss 0.5489\n",
            "Epoch 52 Batch 1000 Loss 0.5519\n",
            "Epoch 52 Batch 1100 Loss 0.5092\n",
            "Epoch 52 Batch 1200 Loss 0.5491\n",
            "Epoch 52 Batch 1300 Loss 0.5198\n",
            "Epoch 52 Batch 1400 Loss 0.6197\n",
            "Epoch 52 Batch 1500 Loss 0.5524\n",
            "Epoch 52 Batch 1600 Loss 0.5416\n",
            "Epoch 52 Batch 1700 Loss 0.5287\n",
            "Epoch 52 Batch 1800 Loss 0.5627\n",
            "Epoch 52 Batch 1900 Loss 0.5833\n",
            "Epoch 52 Batch 2000 Loss 0.5406\n",
            "Epoch 52 Batch 2100 Loss 0.5865\n",
            "Epoch 52 Batch 2200 Loss 0.6048\n",
            "Epoch 52 Batch 2300 Loss 0.5659\n",
            "Epoch 52 Batch 2400 Loss 0.5503\n",
            "Epoch 52 Batch 2500 Loss 0.5595\n",
            "you didn t happen to call the house last night did you grady ?\n",
            "[[2, 84, 12, 430, 9, 143, 7, 254, 175, 159, 67, 2, 1993, 3]]\n",
            "no . <end>\n",
            "How are you today ?\n",
            "[[49, 37, 2, 305, 3]]\n",
            "that i st <end>\n",
            "Epoch 53 Batch 0 Loss 0.5054\n",
            "Epoch 53 Batch 100 Loss 0.5918\n",
            "Epoch 53 Batch 200 Loss 0.4244\n",
            "Epoch 53 Batch 300 Loss 0.4712\n",
            "Epoch 53 Batch 400 Loss 0.5545\n",
            "Epoch 53 Batch 500 Loss 0.5906\n",
            "Epoch 53 Batch 600 Loss 0.5078\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-61702c983caa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/decoder_{}.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_seq_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_initial_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3hC-xs2jIDC"
      },
      "source": [
        "encoder_checkpoint = tf.train.latest_checkpoint(trained_model_path + '/encoder/encoder_31.h5')\n",
        "decoder_checkpoint = tf.train.latest_checkpoint(trained_model_path + '/decoder/')\n",
        "if encoder_checkpoint is not None:\n",
        "  print(trained_model_path + '/encoder')\n",
        "encoder.load_weights(trained_model_path + '/encoder_51.h5')\n",
        "decoder.load_weights(trained_model_path + '/decoder_51.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBOY4LXp-jdJ"
      },
      "source": [
        "encoder.save_weights(trained_model_path\n",
        "              + '/encoder_{}.h5'.format(53 + 1))\n",
        "decoder.save_weights(trained_model_path + '/decoder_{}.h5'.format(53 + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "staKQ1scairF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "9c005522-7fa9-4ad3-8033-189f187b5827"
      },
      "source": [
        "predict('hello')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello\n",
            "[[273]]\n",
            ". . <end>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[[[1.]]],\n",
              " \n",
              " \n",
              "        [[[1.]]],\n",
              " \n",
              " \n",
              "        [[[1.]]]], dtype=float32), ['hello'], ['.', '.', '<end>'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQJzosE_PoLX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}